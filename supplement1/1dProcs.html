<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supplement 1:  Correlated Random Draws</title>

<!-- MathJax -->
  <script>
    window.MathJax = { tex: { tags: "ams" } };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <!-- PrismJS -->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.css" />
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.css" />
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.css" />

  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-matlab.min.js"></script>

  <!-- Your site-wide styling -->
  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/mainPage.css" />
  <link rel="stylesheet" href="../css/article.css" />

  <!-- Your site-wide JS utilities -->
  <script src="../js/layout.js" defer></script>
  <script src="../js/navigation.js" defer></script>
  <script src="../js/loadMatlabFile.js" defer></script>

</head>

<body>
<!-- ===== HEADER ===== -->
<header class="page-header"
        data-title="Supplement 1: Correlated Random Draws with Gaussian PDF">
</header>

  <!-- ===== Sidebar Navigation (populated by navigation.js) ===== -->
  <nav id="right_nav_box" aria-label="Chapter navigation"></nav>

<!-- ===== MAIN CONTENT ===== -->
  <main class="text">

		<h1>1-D Processes</h1>
		<small><i>updated February 24, 2024</i></small>
		<p>
		It can be useful to generate random draws of correlated processes. These can provide an input with known statistics for sensor models, estimation algorithms, control systems, etc. This article provides an introduction to the process, which also serves as background for generating phase screens, as described in Section 9.3. In particular, this article focuses on one-dimensional (<abbr title = "One-Dimensional">1-D</abbr>) processes with 
		a Gaussian probability density function (<abbr title = "Probability Density Function">PDF</abbr>).
		</p>
		
		<h2>Real-Valued Gaussian Random Variables</h2>
		<!--
		Start with real and transition to complex
		independent Gaussian Random variables are also uncorrelated
		-->
		
		<h3>Independent Gaussian Random Variables</h3>
		<p>
		Let \(u_i\) be one variable from a set of \(N\) <dfn>independent random variables</dfn> with a Gaussian <abbr title = "Probability Density Function">PDF</abbr> given by
		$$
		\begin{equation}
			p\left(u_i\right) = \frac{1}{\sqrt{2 \pi \sigma_{ui}^2}} \exp\left[-\frac{\left(u_i-\mu_{ui}\right)^2}{2\sigma_{ui}^2}\right],
		\end{equation}
		$$
		where \(\mu_{ui}\) is the set of \(N\) mean values and \(\sigma_{ui}^2\) is the set of \(N\) variances. The \(N^{th}\)-order joint <abbr title = "Probability Density Function">PDF</abbr> of all of the \(u_i\) is product of individual <abbr title = "Probability Density Functions">PDFs</abbr> given by
		$$
		\begin{equation}
		    p\left(u_1, u_2, \ldots, u_N\right) = \prod\limits_{i=1}^N p\left(u_i\right).
		\end{equation}
		$$
		</p>
		
		<p>
		You can generate independent Gaussian random numbers with a variance of 1 and mean of 0 in Matlab using the <span class="code">randn</span> function. Here is how to adjust the mean and variance: Let \(u\) be a Gaussian random number with a variance of 1 and mean of 0. Then, the number
		$$
		\begin{equation}
		    v = \sigma_v u + \mu_v
		\end{equation}
		$$
		has a Gaussian <abbr title = "Probability Density Function">PDF</abbr> with a mean of \(\mu_v\) and variance of \(\sigma_v^2\).
		</p>
		
		<h3>Correlated Gaussian Random Variables</h3>
		<p>
		Let \(v_i\) be a set of \(N\) <dfn>correlated random variables</dfn>, and denote all of the \(v_i\) as a column vector \(\mathbf{v}\). It has a vector of mean values \(\mathbf{\mu}_{v}\). The elements of the covariance matrix are given by
		$$
		\begin{equation}
		    C_{ij} = \left\langle \left(v_i - \mu_{vi}\right)\left(v_j - \mu_{vj}\right)\right\rangle.
		\end{equation}
		$$
		With these definitions, the \(N^{th}\)-order joint <abbr title = "Probability Density Function">PDF</abbr> of all of the \(v_i\) (or the vector \(\mathbf{v}\)) is
		$$
		\begin{equation}
			p\left(\mathbf{v}\right) = \frac{1}{\left(2\pi\right)^{N/2} \left\vert \mathbf{C} \right\vert^{1/2}} \exp\left[-\frac{1}{2}\left(\mathbf{v}-\mathbf{\mu}_v\right)^t \mathbf{C}^{-1}\left(\mathbf{v}-\mathbf{\mu}_v\right)\right].
		\end{equation}
		$$
		</p>
		
		<h2>Complex-Valued Gaussian Random Variables</h2>
		<h3>General Mean and Covariance</h3>
		<p>
		In this case, each random variable \(z_i\) is complex-values such that
		$$
		\begin{equation}
			z_i = u_i + \text{i}v_i.
		\end{equation}
		$$
		Readers should note the difference between the integer index \(i\) and the imaginary number \(\text{i} = \sqrt{-1}\). Joint <abbr title = "Probability Density Functions">PDFs</abbr> of complex numbers are written in terms of a vector of the real and imaginary parts stacked on top of each other like
		$$
		\begin{equation}
			\mathbf{z} =
			\begin{pmatrix}
			u_1 \\
			u_2 \\
			\vdots \\
			u_N \\
			v_1 \\
			v_2 \\
			\vdots \\
			v_N
			\end{pmatrix}.
		\end{equation}
		$$
		This is a vector of length \(2 N\). For a complex Gaussian random vector, the form of the <abbr title = "Probability Density Function">PDF</abbr> does not change. Namely, it is still written as
		$$
		\begin{equation}
			p\left(\mathbf{z}\right) = \frac{1}{\left(2\pi\right)^{N/2} \left\vert \mathbf{C} \right\vert^{1/2}} \exp\left[-\frac{1}{2}\left(\mathbf{z}-\mathbf{\mu}_z\right)^t \mathbf{C}^{-1}\left(\mathbf{z}-\mathbf{\mu}_z\right)\right],
		\end{equation}
		$$
		where \(\mathbf{\mu}_z = \left\langle \mathbf{z} \right\rangle\) is the mean vector, and \(\mathbf{C}\) is the \(2 N \times 2 N\) covariance matrix of \(\mathbf{z}\).
		</p>
		
		<h3>Circular Complex Gaussian Random Variables</h3>
		<p>
		We can define a special class for Gaussian random variables called <abbr title = "Circular Complex Gaussian">CCG</abbr>. To define the conditions for <abbr title = "Circular Complex Gaussian">CCG</abbr>, we define two separate vectors from the real and imaginary parts according to
		$$
		\begin{align}
			\mathbf{u} &=
			\begin{pmatrix}
				u_1 \\
				u_2 \\
				\vdots \\
				u_N
			\end{pmatrix}, & 
			\mathbf{v} &=
			\begin{pmatrix}
				v_1 \\
				v_2 \\
				\vdots \\
				v_N
			\end{pmatrix}.
		\end{align}
		$$
		Additionally, we define their covariance matrices by
		$$
		\begin{align}
			\mathbf{C}^{uu} & = \left\langle \left(\mathbf{u}-\mathbf{\mu}_u\right) \left(\mathbf{u}-\mathbf{\mu}_u\right)^t \right\rangle &
			\mathbf{C}^{vv} & = \left\langle \left(\mathbf{v}-\mathbf{\mu}_v\right) \left(\mathbf{v}-\mathbf{\mu}_v\right)^t \right\rangle \\
			\mathbf{C}^{uv} & = \left\langle \left(\mathbf{u}-\mathbf{\mu}_u\right) \left(\mathbf{v}-\mathbf{\mu}_v\right)^t \right\rangle &
			\mathbf{C}^{vu} & = \left\langle \left(\mathbf{v}-\mathbf{\mu}_v\right) \left(\mathbf{u}-\mathbf{\mu}_u\right)^t \right\rangle.
		\end{align}
		$$
		The special case of <abbr title = "Circular Complex Gaussian">CCG</abbr> is defined by zero-valued mean vectors
		$$
		\begin{equation}
			\mathbf{\mu}_u = \mathbf{\mu}_v = \mathbf{0}
		\end{equation}
		$$
		and symmetry in the covariance matrices such that
		$$
		\begin{align}
			\mathbf{C}^{uu} &= \mathbf{C}^{uu}, & \mathbf{C}^{uv} &= -\mathbf{C}^{vu}.
		\end{align}
		$$
		</p>
		
		<h2>Sums of Gaussian Random Variables</h2>
		<p>
		Gaussian random variables have the property that weighted sums of Gaussian variables still have a Gaussian <abbr title = "Probability Density Function">PDF</abbr>. For example, let \(a_i\) be a set of \(N\) fixed numbers. Then, the sum
		$$
		\begin{equation}
			w = \sum\limits_{i=1}^N a_i u_i,
		\end{equation}
		$$
		also has a Gaussian <abbr title = "Probability Density Function">PDF</abbr> given by
		$$
		\begin{equation}
			\label{eq:GaussSumPDF}
			p\left(w\right) = \frac{1}{\sqrt{2 \pi \sigma_w^2}} \exp\left[-\frac{\left(w-\mu_w\right)^2}{2\sigma_w^2}\right].
		\end{equation}
		$$
		In this equation, the \(w\) parameters are related to the \(u_i\) parameters acccording to
		$$
		\begin{align}
		    \mu_w = \sum\limits_{i=1}^N a_i \mu_{ui} \\
			\sigma_w^2 = \sum\limits_{i=1}^N a_i \sigma_{ui}^2.
		\end{align}
		$$
		Equation \(\eqref{eq:GaussSumPDF}\) is an important property of working with Gaussian random numbers. Random variables with other <abbr title = "Probability Density Functions">PDFs</abbr> have different relationships. For more information on these relationships, see the Wikipedia citations below.
		</p>
		
		<p>
		The sum of random variable above can be extended to a vector of Gaussian random variables in the following way. Let \(\mathbf{u}\) be column vector of \(N\) <dfn>independent random variables</dfn> \(u_j\) with the same Gaussian <abbr title = "Probability Density Function">PDF</abbr> that has a mean of 0 and variance of 1.	Also, let \(\mathbf{A}\) be a matrix with \(M\) rows and \(N\) columns of fixed values \(a_{ij}\). Then, the random column vector
		$$
		\begin{equation}
			\mathbf{w} = \mathbf{A} \mathbf{u}
		\end{equation}
		$$
		has \(M\) values. The entries in \(\mathbf{w}\) are Gaussian random variables that have a mean of zero and a covariance given by \(\mathbf{C} = \mathbf{A} \mathbf{A}^t\).
		</p>		
		<!--
		Look up section on colored random noise in green book from Goda
		Look up section on Zernike phase screens in Roggemann & Welsh
		Look up my paper on phase screens with SLMs and citations
		Find code example on laptop
		
		<p>
		Joint PDF for two correlated random variables. Example of generating two correlated random variables.
		</p>
		
		<p>
		Joint PDF for \(N\) correlated random variables. R and W method with Cholesky decomposition.
		</p>

		<p>
		If we know the covariance, we also know the PSD, In optical turbulence, we most often know the PSD, so focus on this method. Explain in terms of code. Show example usage with plots of PDF, mean, and covariance.
		</p>
		-->
		<h2>References</h2>
		<ol>
			<li>Joseph W. Goodman, <u>Statistical Optics</u> Ch. 3, Wiley, New York, NY (1985)</li>
			<li><a href="https://en.wikipedia.org/wiki/Relationships_among_probability_distributions">Relationships among probability distributions</a>, In <i>Wikipedia.</i></li>
			<li><a href="https://en.wikipedia.org/wiki/Algebra_of_random_variables">Algebra of random variables</a>, In <i>Wikipedia.</i></li>
			<li><a href="https://en.wikipedia.org/wiki/List_of_convolutions_of_probability_distributions">List of convolutions of probability distributions</a>, In <i>Wikipedia.</i></li>
		</ol>
  </main>

  <!-- ===== FOOTER ===== -->
  <footer id="site_footer"></footer>

</body>
</html>